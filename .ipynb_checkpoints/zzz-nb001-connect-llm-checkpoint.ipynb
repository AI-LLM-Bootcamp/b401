{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Connect with an LLM\n",
    "* Start talking with ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759d27a-94f3-4141-945d-065f2095bffd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Intro\n",
    "* Input: the prompt we send to the LLM.\n",
    "* Output: the response from the LLM.\n",
    "* We can switch LLMs and use several different LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332b6e9-8164-4859-879c-f021a4dfd69d",
   "metadata": {},
   "source": [
    "## LangChain divides LLMs in two types\n",
    "1. LLM Model: text-completion model.\n",
    "2. Chat Model: converses with a sequence of messages and can have a particular role defined (system prompt). This type has become the most used in LangChain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42a3ca-fc4d-4b91-b3bc-a7304ec4d5f8",
   "metadata": {},
   "source": [
    "## See the differences\n",
    "* Even when sometimes the LangChain documentation can be confusing about it, the fact is that text-completion models and Chat models are both LLMs.\n",
    "* But, as you can see in this [playground](https://platform.openai.com/playground/chat?models=gpt-4o), they have some significant differences. See that the chat models in LangChain have system messages, human messages (called \"user messages\" by OpenAI) and AI messages (called \"Assitant Messages\" by OpenAI).\n",
    "* Since the launch of chatGPT, the Chat Model is the most popular LLM type and is used in most LLM apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f9501-fa9e-4830-95e2-537dff951cf1",
   "metadata": {},
   "source": [
    "## List of LLMs that can work with LangChain\n",
    "* See the list [here](https://python.langchain.com/v0.1/docs/integrations/llms/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80616acf-ae85-4226-ba19-dbbbb9d4796f",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a6725-81b3-4ea3-b39a-eb8f2ded91a2",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 001-connect-llm.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-connect-llm**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2746c97-9fa5-481c-8333-21de1504a087",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a10870-432e-4818-aa5e-6be24c579d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa888f7-3718-4829-8645-30acb43db51f",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d5b71-b26a-4cd5-9765-019077a67141",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27",
   "metadata": {},
   "source": [
    "## LLM Model\n",
    "* The trend before the launch of chatGPT-4.\n",
    "* See LangChain documentation about LLM Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92628f2-62e8-436c-92d4-e849de7744ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llmModel = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeaf300-5cb6-4df8-bcb9-1fe3c82e4b22",
   "metadata": {},
   "source": [
    "#### Invoke: all the text of the reponse is printed at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ade5fa-b487-4f32-bda7-0f9c41b096b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llmModel.invoke(\n",
    "    \"Tell me one fun fact about the Kennedy family.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33285ef-4bb5-4fd6-9894-790c07a5468c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nOne fun fact about the Kennedy family is that they have their own personalized crest, which includes symbols such as a lion, an eagle, and a ship's wheel, representing courage, strength, and leadership.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa71043-4707-44b4-8d1f-8540d5d139b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One fun fact about the Kennedy family is that they have their own personalized crest, which includes symbols such as a lion, an eagle, and a ship's wheel, representing courage, strength, and leadership.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c29ae-d60a-44c7-925c-20084c0b941e",
   "metadata": {},
   "source": [
    "#### Streaming: printing one chunk of text at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "158aa111-9167-444c-9b10-90b095eac390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One fun fact about the Kennedy family is that President John F. Kennedy's favorite meal was New England fish chowder. He often requested it for meals at the White House and even had a special recipe created just for him."
     ]
    }
   ],
   "source": [
    "for chunk in llmModel.stream(\n",
    "    \"Tell me one fun fact about the Kennedy family.\"\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98101f10-6349-4f98-9fb7-a1400166cedc",
   "metadata": {},
   "source": [
    "#### Temperature: more or less creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac12473a-1ffd-4a22-852a-92e1e6374914",
   "metadata": {},
   "outputs": [],
   "source": [
    "creativeLlmModel = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "325b9b2a-c61d-4264-81bf-71090f9b0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llmModel.invoke(\n",
    "    \"Write a short 5 line poem about JFK\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "679c1220-13ed-4251-ba4b-f8f1ba350f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "A leader with grace and charm,\n",
      "His words inspired and disarmed,\n",
      "JFK, a symbol of hope,\n",
      "With a vision beyond the scope,\n",
      "Forever remembered, his legacy will never depart.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31346a0c-ee9c-4a80-8e8a-85ef682df7c3",
   "metadata": {},
   "source": [
    "## Chat Model\n",
    "* The general trend after the launch of chatGPT-4.\n",
    "    * Frequently known as \"Chatbot\". \n",
    "    * Conversation between Human and AI.\n",
    "    * Can have a system prompt defining the tone or the role of the AI. \n",
    "* See LangChain documentation about Chat Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/).\n",
    "* By default we will work with ChatOpenAI. See [here](https://python.langchain.com/v0.1/docs/integrations/chat/openai/) the LangChain documentation page about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eaa967e-067f-4574-8cbc-b1e5b7956fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are an historian expert in the Kennedy family.\"),\n",
    "    (\"human\", \"Tell me one curious thing about JFK.\"),\n",
    "]\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f567f5a-c4ea-4234-84e6-422bf6104589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"One curious thing about JFK is that he was a collector of unique and eclectic items. He had a fascination with history and art, and his collection included everything from ship models and scrimshaw to original manuscripts and rare books. JFK's love of collecting even extended to quirky items like coconut shells carved with faces that he displayed in the Oval Office. This hobby provided a glimpse into his personal interests and served as a reflection of his intellectual curiosity.\", response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 29, 'total_tokens': 116}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b2f90721-52fe-4482-989a-bc75d6f3e7fd-0', usage_metadata={'input_tokens': 29, 'output_tokens': 87, 'total_tokens': 116})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19a6abd4-a072-47ea-ae60-7959def9b602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One curious thing about JFK is that he was a collector of unique and eclectic items. He had a fascination with history and art, and his collection included everything from ship models and scrimshaw to original manuscripts and rare books. JFK's love of collecting even extended to quirky items like coconut shells carved with faces that he displayed in the Oval Office. This hobby provided a glimpse into his personal interests and served as a reflection of his intellectual curiosity.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47927ac1-13bc-40eb-9917-9d39418f0f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 87,\n",
       "  'prompt_tokens': 29,\n",
       "  'total_tokens': 116},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7847143f-3179-423e-b499-a6e494967741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'AIMessage',\n",
       " 'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       " 'type': 'object',\n",
       " 'properties': {'content': {'title': 'Content',\n",
       "   'anyOf': [{'type': 'string'},\n",
       "    {'type': 'array',\n",
       "     'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "  'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "  'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "  'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'},\n",
       "  'name': {'title': 'Name', 'type': 'string'},\n",
       "  'id': {'title': 'Id', 'type': 'string'},\n",
       "  'example': {'title': 'Example', 'default': False, 'type': 'boolean'},\n",
       "  'tool_calls': {'title': 'Tool Calls',\n",
       "   'default': [],\n",
       "   'type': 'array',\n",
       "   'items': {'$ref': '#/definitions/ToolCall'}},\n",
       "  'invalid_tool_calls': {'title': 'Invalid Tool Calls',\n",
       "   'default': [],\n",
       "   'type': 'array',\n",
       "   'items': {'$ref': '#/definitions/InvalidToolCall'}},\n",
       "  'usage_metadata': {'$ref': '#/definitions/UsageMetadata'}},\n",
       " 'required': ['content'],\n",
       " 'definitions': {'ToolCall': {'title': 'ToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'type': {'title': 'Type', 'enum': ['tool_call'], 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id']},\n",
       "  'InvalidToolCall': {'title': 'InvalidToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'error': {'title': 'Error', 'type': 'string'},\n",
       "    'type': {'title': 'Type',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error']},\n",
       "  'UsageMetadata': {'title': 'UsageMetadata',\n",
       "   'type': 'object',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens']}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54631efb-dbe2-4bd7-b588-324f19a28b2c",
   "metadata": {},
   "source": [
    "#### Before the previous one, the old way (but still very popular) of doing this was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a20ba943-ce6b-45e2-9dbb-351c48421069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b879a5f4-2250-4a70-b065-7796c4fe9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an historian expert on the Kennedy Family.\"),\n",
    "    HumanMessage(content=\"How many children had Joseph P. Kennedy?\"),\n",
    "]\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78f9b26b-e7e1-4613-8e3e-0c81f8b54da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Joseph P. Kennedy and his wife Rose Fitzgerald Kennedy had nine children. Their children were Joseph Jr., John F. (JFK), Rosemary, Kathleen, Eunice, Patricia, Robert F. (Bobby), Jean, and Edward M. (Ted).', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-63964416-759f-461a-aecb-789b336ce0c1-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41204f4-0cec-4039-a18d-2a25b9dd5efd",
   "metadata": {},
   "source": [
    "#### Streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83d9f1d0-bba8-4885-9ac1-01fc273732b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy and his wife Rose Kennedy had nine children: Joseph Jr., John F. (JFK), Rosemary, Kathleen, Eunice, Patricia, Robert (Bobby), Jean, and Edward (Ted)."
     ]
    }
   ],
   "source": [
    "for chunk in chatModel.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd276b9-494a-47c2-8455-2ff33fc4f221",
   "metadata": {},
   "source": [
    "#### Another old way, similar results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bc0210b-27e0-41ed-8ad8-ff1c75c9bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are expert {profession} in {topic}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chatModel\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"profession\": \"Historian\",\n",
    "        \"topic\": \"Kennedy Family\",\n",
    "        \"input\": \"Tell me one fun fact about JFK.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc436d53-b372-43eb-ac00-ee39c9fa491a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='One fun fact about JFK is that he was the first president to hold a press conference that was broadcast live on television. This event took place on January 25, 1961, and it allowed the American public to see and hear their president in real-time, marking a significant shift in how political communication was conducted.', response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 28, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3e6f4f2c-a7fe-4220-925e-1e3a6954cc57-0', usage_metadata={'input_tokens': 28, 'output_tokens': 64, 'total_tokens': 92})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 001-connect-llms.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 001-connect-llm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
