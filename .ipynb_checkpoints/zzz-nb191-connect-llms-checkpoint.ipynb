{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Connecting with the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759d27a-94f3-4141-945d-065f2095bffd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Intro\n",
    "* Input: the prompt we send to the LLM.\n",
    "* Output: the response from the LLM.\n",
    "* We can switch LLMs and use several different LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981f91b-686d-401c-a872-65487f93b46e",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* LLMs.\n",
    "* Prompts and Prompt Templates.\n",
    "* Types of prompts: Zero Shot and Few Shot(s) Prompt.\n",
    "* Serialization: Saving and Loading Prompts.\n",
    "* Parsing Outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332b6e9-8164-4859-879c-f021a4dfd69d",
   "metadata": {},
   "source": [
    "## LangChain divides LLMs in two types\n",
    "1. LLM Model: text-completion model.\n",
    "2. Chat Model: converses with a sequence of messages and can have a particular role defined (system prompt). This type has become the most used in LangChain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42a3ca-fc4d-4b91-b3bc-a7304ec4d5f8",
   "metadata": {},
   "source": [
    "## See the differences\n",
    "* Even when sometimes the LangChain documentation can be confusing about it, the fact is that text-completion models and Chat models are both LLMs.\n",
    "* But, as you can see in this [playground](https://platform.openai.com/playground/chat?models=gpt-4o), they have some significant differences. See that the chat models in LangChain have system messages, human messages (called \"user messages\" by OpenAI) and AI messages (called \"Assitant Messages\" by OpenAI).\n",
    "* Since the launch of chatGPT, the Chat Model is the most popular LLM type and is used in most LLM apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f9501-fa9e-4830-95e2-537dff951cf1",
   "metadata": {},
   "source": [
    "## List of LLMs that can work with LangChain\n",
    "* See the list [here](https://python.langchain.com/v0.1/docs/integrations/llms/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80616acf-ae85-4226-ba19-dbbbb9d4796f",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a6725-81b3-4ea3-b39a-eb8f2ded91a2",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the b401-main.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **b401-connectWithLLMs**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2746c97-9fa5-481c-8333-21de1504a087",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a10870-432e-4818-aa5e-6be24c579d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa888f7-3718-4829-8645-30acb43db51f",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d5b71-b26a-4cd5-9765-019077a67141",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27",
   "metadata": {},
   "source": [
    "## LLM Model\n",
    "* The trend before the launch of chatGPT-4.\n",
    "* See LangChain documentation about LLM Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92628f2-62e8-436c-92d4-e849de7744ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llmModel = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeaf300-5cb6-4df8-bcb9-1fe3c82e4b22",
   "metadata": {},
   "source": [
    "#### Invoke: all the text of the reponse is printed at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ade5fa-b487-4f32-bda7-0f9c41b096b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llmModel.invoke(\n",
    "    \"Tell me one fun fact about the Kennedy family.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33285ef-4bb5-4fd6-9894-790c07a5468c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nOne fun fact about the Kennedy family is that they have a tradition of playing touch football on Thanksgiving Day. This tradition began with John F. Kennedy and his siblings when they were children and has continued through the generations.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa71043-4707-44b4-8d1f-8540d5d139b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One fun fact about the Kennedy family is that they have a tradition of playing touch football on Thanksgiving Day. This tradition began with John F. Kennedy and his siblings when they were children and has continued through the generations.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c29ae-d60a-44c7-925c-20084c0b941e",
   "metadata": {},
   "source": [
    "#### Streaming: printing one chunk of text at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "158aa111-9167-444c-9b10-90b095eac390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One fun fact about the Kennedy family is that President John F. Kennedy's sister, Eunice Kennedy Shriver, founded the Special Olympics in 1968. She was inspired to create the organization by her sister, Rosemary Kennedy, who had an intellectual disability."
     ]
    }
   ],
   "source": [
    "for chunk in llmModel.stream(\n",
    "    \"Tell me one fun fact about the Kennedy family.\"\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98101f10-6349-4f98-9fb7-a1400166cedc",
   "metadata": {},
   "source": [
    "#### Temperature: more or less creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac12473a-1ffd-4a22-852a-92e1e6374914",
   "metadata": {},
   "outputs": [],
   "source": [
    "creativeLlmModel = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "325b9b2a-c61d-4264-81bf-71090f9b0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llmModel.invoke(\n",
    "    \"Write a short 5 line poem about JFK\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "679c1220-13ed-4251-ba4b-f8f1ba350f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In Camelot he reigned,\n",
      "Youthful and full of grace,\n",
      "A leader of hope and change,\n",
      "His legacy forever ingrained,\n",
      "JFK, forever in our hearts and minds.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31346a0c-ee9c-4a80-8e8a-85ef682df7c3",
   "metadata": {},
   "source": [
    "## Chat Model\n",
    "* The general trend after the launch of chatGPT-4.\n",
    "    * Frequently known as \"Chatbot\". \n",
    "    * Conversation between Human and AI.\n",
    "    * Can have a system prompt defining the tone or the role of the AI. \n",
    "* See LangChain documentation about Chat Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/).\n",
    "* By default we will work with ChatOpenAI. See [here](https://python.langchain.com/v0.1/docs/integrations/chat/openai/) the LangChain documentation page about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eaa967e-067f-4574-8cbc-b1e5b7956fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are an historian expert in the Kennedy family.\"),\n",
    "    (\"human\", \"Tell me one curious thing about JFK.\"),\n",
    "]\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f567f5a-c4ea-4234-84e6-422bf6104589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='One curious thing about John F. Kennedy is that he won a Pulitzer Prize in 1957 for his book \"Profiles in Courage,\" which he wrote while serving as a U.S. Senator. It is interesting because Kennedy is the only U.S. President to have won a Pulitzer Prize.', response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 29, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cb8a5ba1-2426-4a0b-9ef6-dd30d9c7fc06-0', usage_metadata={'input_tokens': 29, 'output_tokens': 58, 'total_tokens': 87})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19a6abd4-a072-47ea-ae60-7959def9b602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious thing about John F. Kennedy is that he won a Pulitzer Prize in 1957 for his book \"Profiles in Courage,\" which he wrote while serving as a U.S. Senator. It is interesting because Kennedy is the only U.S. President to have won a Pulitzer Prize.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47927ac1-13bc-40eb-9917-9d39418f0f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 58,\n",
       "  'prompt_tokens': 29,\n",
       "  'total_tokens': 87},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7847143f-3179-423e-b499-a6e494967741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'AIMessage',\n",
       " 'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       " 'type': 'object',\n",
       " 'properties': {'content': {'title': 'Content',\n",
       "   'anyOf': [{'type': 'string'},\n",
       "    {'type': 'array',\n",
       "     'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "  'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "  'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "  'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'},\n",
       "  'name': {'title': 'Name', 'type': 'string'},\n",
       "  'id': {'title': 'Id', 'type': 'string'},\n",
       "  'example': {'title': 'Example', 'default': False, 'type': 'boolean'},\n",
       "  'tool_calls': {'title': 'Tool Calls',\n",
       "   'default': [],\n",
       "   'type': 'array',\n",
       "   'items': {'$ref': '#/definitions/ToolCall'}},\n",
       "  'invalid_tool_calls': {'title': 'Invalid Tool Calls',\n",
       "   'default': [],\n",
       "   'type': 'array',\n",
       "   'items': {'$ref': '#/definitions/InvalidToolCall'}},\n",
       "  'usage_metadata': {'$ref': '#/definitions/UsageMetadata'}},\n",
       " 'required': ['content'],\n",
       " 'definitions': {'ToolCall': {'title': 'ToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'type': {'title': 'Type', 'enum': ['tool_call'], 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id']},\n",
       "  'InvalidToolCall': {'title': 'InvalidToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'error': {'title': 'Error', 'type': 'string'},\n",
       "    'type': {'title': 'Type',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error']},\n",
       "  'UsageMetadata': {'title': 'UsageMetadata',\n",
       "   'type': 'object',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens']}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54631efb-dbe2-4bd7-b588-324f19a28b2c",
   "metadata": {},
   "source": [
    "#### Before the previous one, the old way (but still very popular) of doing this was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a20ba943-ce6b-45e2-9dbb-351c48421069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b879a5f4-2250-4a70-b065-7796c4fe9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an historian expert on the Kennedy Family.\"),\n",
    "    HumanMessage(content=\"How many children had Joseph P. Kennedy?\"),\n",
    "]\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78f9b26b-e7e1-4613-8e3e-0c81f8b54da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Joseph P. Kennedy and his wife, Rose Fitzgerald Kennedy, had nine children. Their children were Joseph Jr., John F. (Jack), Rosemary, Kathleen, Eunice, Patricia, Robert F. (Bobby), Jean, and Edward M. (Ted).', response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 30, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aee45208-c13c-4736-89aa-fe9be9875edb-0', usage_metadata={'input_tokens': 30, 'output_tokens': 55, 'total_tokens': 85})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41204f4-0cec-4039-a18d-2a25b9dd5efd",
   "metadata": {},
   "source": [
    "#### Streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83d9f1d0-bba8-4885-9ac1-01fc273732b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy and his wife Rose Kennedy had nine children together. Their children were Joseph Jr., John F. (JFK), Rosemary, Kathleen, Eunice, Patricia, Robert F. (RFK), Jean, and Edward M. (Ted)."
     ]
    }
   ],
   "source": [
    "for chunk in chatModel.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd276b9-494a-47c2-8455-2ff33fc4f221",
   "metadata": {},
   "source": [
    "#### Another old way, similar results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bc0210b-27e0-41ed-8ad8-ff1c75c9bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are expert {profession} in {topic}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chatModel\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"profession\": \"Historian\",\n",
    "        \"topic\": \"Kennedy Family\",\n",
    "        \"input\": \"Tell me one fun fact about JFK.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc436d53-b372-43eb-ac00-ee39c9fa491a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='One fun fact about JFK is that he was the first president to hold a press conference that was broadcast live on television. This event took place on January 25, 1961, and it set a precedent for future presidents to directly address the American public through the medium of television.', response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 28, 'total_tokens': 85}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-561276b9-c465-4001-985b-d0741ae2ab53-0', usage_metadata={'input_tokens': 28, 'output_tokens': 57, 'total_tokens': 85})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f91601-8594-41d3-9316-d51791fc54e8",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "* See the LangChain documentation about prompts [here](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/).\n",
    "* Input into LLMs.\n",
    "* Prompt templates: easier to use prompts with variables. A prompt template may include:\n",
    "    * instructions,\n",
    "    * few-shot examples,\n",
    "    * and specific context and questions appropriate for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5383c7e-7e62-46d0-8bef-17ff45a88495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nOne curious story about the Kennedy family involves a supposed curse that has been linked to their family. The curse is said to have started with the family's patriarch, Joseph P. Kennedy, who made his fortune through questionable means and had several affairs. This allegedly angered an Irish mob boss, who cursed the Kennedy family for generations to come.\\n\\nThe curse is said to have claimed the lives of several members of the Kennedy family, including President John F. Kennedy and his brother, Robert F. Kennedy, who were both assassinated in the 1960s. It also claimed the lives of several other family members, including John and Robert's sister, Kathleen Kennedy, who died in a plane crash, and their brother, Joseph P. Kennedy Jr., who died in World War II.\\n\\nThe curse has also been linked to other tragedies in the family, such as the death of John and Robert's nephew, John F. Kennedy Jr., in a plane crash, and the death of their sister, Rosemary Kennedy, who underwent a lobotomy that left her severely disabled.\\n\\nSome believe that the curse has been lifted with the passing of time, but others still believe that the Kennedy family is plagued by bad luck. The curse has become a topic of fascination and speculation, with some even\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story about {topic}.\"\n",
    ")\n",
    "\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective=\"curious\", \n",
    "    topic=\"the Kennedy family\"\n",
    ")\n",
    "\n",
    "llmModel.invoke(llmModelPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beb7dbcd-524c-4acc-9b65-0b209a170ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"Historian\",\n",
    "    topic=\"The Kennedy family\",\n",
    "    user_input=\"How many grandchildren had Joseph P. Kennedy?\"\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7585be48-9a19-43ab-a275-8ef6a04246a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Joseph P. Kennedy, the father of President John F. Kennedy, had a total of 34 grandchildren.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 55, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-93276a79-35a9-4666-837e-e7799090a57e-0', usage_metadata={'input_tokens': 55, 'output_tokens': 22, 'total_tokens': 77})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1373ca0b-342f-4ac6-a697-5e2ad30bfa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Joseph P. Kennedy, the father of President John F. Kennedy, had a total of 34 grandchildren.' response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 55, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-93276a79-35a9-4666-837e-e7799090a57e-0' usage_metadata={'input_tokens': 55, 'output_tokens': 22, 'total_tokens': 77}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f6a5e25-e177-4d7a-ba29-6834a6a8152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy, the father of President John F. Kennedy, had a total of 34 grandchildren.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d770b4-80c5-49a6-a925-de3a800d19f2",
   "metadata": {},
   "source": [
    "#### Old way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6dafd17-47a2-4169-992e-76ffb9702d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are an Historian expert on the Kennedy family.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    user_input=\"Name the children and grandchildren of Joseph P. Kennedy?\"\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a4d5a5a-c3a3-48be-8b31-40b0881faa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy and his wife Rose Kennedy had nine children:\n",
      "\n",
      "1. Joseph P. Kennedy Jr.\n",
      "2. John F. Kennedy\n",
      "3. Rosemary Kennedy\n",
      "4. Kathleen Kennedy\n",
      "5. Eunice Kennedy\n",
      "6. Patricia Kennedy\n",
      "7. Robert F. Kennedy\n",
      "8. Jean Kennedy\n",
      "9. Edward M. Kennedy\n",
      "\n",
      "Some of the grandchildren of Joseph P. Kennedy include:\n",
      "- Caroline Kennedy (daughter of John F. Kennedy)\n",
      "- John F. Kennedy Jr. (son of John F. Kennedy)\n",
      "- Robert F. Kennedy Jr. (son of Robert F. Kennedy)\n",
      "- Maria Shriver (daughter of Eunice Kennedy)\n",
      "- Joseph P. Kennedy II (son of Robert F. Kennedy)\n",
      "- Patrick J. Kennedy (son of Edward M. Kennedy)\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b510dd0-33b7-4737-aef2-c52a1e8bbf41",
   "metadata": {},
   "source": [
    "#### What is the full potential of ChatPromptTemplate?\n",
    "* Check the [corresponding page](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) in the LangChain API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8be09-99fc-491e-babb-0b8ad5bc74c2",
   "metadata": {},
   "source": [
    "## Few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "301ae08a-e223-4239-8eca-8468ceb1e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b882accd-e3e7-4e8a-9d85-55ebaa945837",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b973d828-0f47-4c43-98ab-e3db306d41e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¿Quién fue JFK?\\n\\n', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 52, 'total_tokens': 58}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-08f4a6ce-78c0-44f3-843e-040b83096d2e-0', usage_metadata={'input_tokens': 52, 'output_tokens': 6, 'total_tokens': 58})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chatModel\n",
    "\n",
    "chain.invoke({\"input\": \"Who was JFK?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721c4a4-56d6-48e2-b9db-0de0309c492a",
   "metadata": {},
   "source": [
    "## Parsing Outputs\n",
    "* See the corresponding LangChain Documentation page [here](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/).\n",
    "* Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "177b19be-ea45-4ecb-a4dc-914da7958de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "json_chain = json_prompt | llmModel | json_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb4ed2-f174-4177-b38d-b03397e4f2a7",
   "metadata": {},
   "source": [
    "#### The previous prompt template includes the parser instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "047f7986-5502-4205-9a7a-e7e7e26f51a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecb755d4-4a0f-486a-99dd-ea1873eb7c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ['Russia', 'Canada', 'China']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_chain.invoke({\"question\": \"List the 3 biggest countries\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1181d87-13df-4abf-9910-0b44618642de",
   "metadata": {},
   "source": [
    "#### Optionally, you can use Pydantic to define a custom output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdcf2860-9b94-46f9-94f8-81ce173b8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "873de824-c105-4e36-b4a7-d4bb498efc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7006219-e78f-4f1f-8d8a-559679f84845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle find its way home?\",\n",
       " 'punchline': 'Because it lost its bearings!'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up a parser\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Inject parser instructions into the prompt template.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Create a chain with the prompt and the parser\n",
    "chain = prompt | chatModel | parser\n",
    "\n",
    "chain.invoke({\"query\": \"Tell me a joke.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 001-connect-llms.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 001-connect-llms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
